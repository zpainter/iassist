+++
year = "2025"
draft = "false"
title = "IASSIST 2025 - Workshops"
location = "Bristol, UK"
banner = "/img/conferences/annual/bristol2025-socialmedia.jpg"
type = "conference-2025"
+++
<!--
## Workshop registration

You may now register for IASSIST & CARTO 2024 Workshops. Workshops will be held on Tuesday May 28 at Dalhousie University. More information about room locations will be provided in advance. 

It is only possible to attend one workshop in the morning and another in the afternoon. Workshop registration is open to all, you do not have to register for the full conference to register for a workshop. 

<br />
<a class="btn btn-template-main" href="https://iassistdata.info/event-5601452">Register for IASSIST & CARTO Workshop <span class="fas fa-external-link-alt"></span></a>
<br /><br />

**Please note** that due to a system limitation, you can only register for 1 workshop at a time. If you wish to register for two workshops (both morning and afternoon), complete the workshop registration form twice. We apologize for any inconvenience.

### Information for participants

Once you have registered for a workshop, please see this [information document <span class="fas fa-external-link-alt"></span>](https://tinyurl.com/iassist24workshops) with information regarding what you should bring with you to the workshop. Instructors may also contact participants prior to the workshop with more details. 
-->

## Workshops available

The workshops run on Tuesday the 3rd of June and are supplementary to the conference. 

Each workshop costs £50 to attend. The workshops will run in parallel so can register for up to one in the morning and one in the afternoon.

The workshops will be in person, held at [UWE Bristol Frenchay campus <span class="fas fa-external-link-alt"></span>](https://www.uwe.ac.uk/life/campus-and-facilities/frenchay-campus) rather than at the main conference venue, they are 3 hour sessions each, to keep costs as low as possible they don’t include catering but there are plenty of catering outlets on campus. 

The workshops have limited availability. Although you can register to them at a later date it is advised that if there is a workshop you are particularly keen on attending you should register to it at the time of your booking to avoid disappointment. 

<!--
You can register for the workshops separately to the [main registration <span class="fas fa-external-link-alt"></span>](https://store.uwe.ac.uk/conferences-and-events/research-business-and-innovation/iassist/iassist-international-association-for-social-science-information-service-and-technology).
-->

### The morning workshops are:

1. Bridging Design and Accessibility: Creating FAIR and Inclusive Visualizations
2. Teaching Qualitative Data Analysis using Open Data, Standards, and Tools
3. AI-enabled data practices for metadata discovery and access: Best practices for developing training data
4. Learn how to create synthetic data

### The Afternoon workshops are:

5. Embedding sensitive data management best practice in institutional workflows
6. The Art of Transcription: Using open-source tools to optimize transcription processes
7. Fundamentals of MAST & IDEAL Metadata
8. An Introduction to Census Data with R Using the "tidycensus" Package

## Morning Workshops

### 1: Bridging Design and Accessibility: Creating FAIR and Inclusive Visualizations

This interactive workshop is aimed at making visualizations more appealing, accessible, and FAIR. Participants will explore strategies to determine the accessibility of a graph, learn tips for creating accessible graphs, and create their own visualizations with various tools. Beginning with the theory of design then diving into individual elements with hands-on exercises, participants will experience the full data visualization cycle and practice creating accessible graphs using software such as Excel, Tableau, RStudio, and Jupyter Notebooks. While all of these software are open-source and/or freely available (except Excel which is fairly ubiquitous), they differ significantly in how they visualize graphs, requiring distinct approaches towards accessibility. In addition, participants will learn how to create accessible tables and documentation to ensure a fully FAIR package that can be understood and further built upon by the public.

As data sharing becomes increasingly prevalent, it is imperative that data professionals become knowledgeable on how visualizations can be shared in a way that is FAIR, but also accessible to individuals with disabilities. Raw data and code receive the most attention when it comes to ensuring research is FAIR, but visualizations should also receive the same treatment. While data and code increase the reproducibility of research, it is the visualizations that make it understandable to the general public. This workshop will help bridge this gap and encourage data professionals to expand the range of research outputs they assist researchers in producing.

**Workshop Learning Objectives:**

1. Participants will be able to evaluate if a graph is accessible. 
2. Participants will be able to design accessible visualizations in various tools such as Excel, Tableau, RStudio, and Jupyter Notebooks.  
3. Participants will be able to write documentation and create tables to ensure their visualizations and associated materials are accessible and FAIR. 

**Workshop Prerequisites:**

The workshop is open to everyone regardless of programming or visualization experience. Participants are expected to download Excel, Tableau (either Public Desktop - free version or Desktop are fine), RStudio, and Jupyter Notebooks onto their laptops. These software (besides Excel) are all open and free. The workshop instructors will email downloading instructions to registered participants before the workshop/conference begins. The instructors are happy to answer any troubleshooting questions before the workshop begins or during the first few minutes.

**Workshop Setting:**

Classroom - participants bring laptops

**Workshop Computer Requirements:**

Windows OS are preferred but others are also fine. If a computer lab has all necessary software [Excel, Tableau, RStudio, and Jupyter Notebooks], this is preferable to laptops as participants will not need to download software.

### 2: Teaching Qualitative Data Analysis using Open Data, Standards, and Tools

Teaching Computer-Assisted Qualitative Data Analysis (CAQDAS) can be a challenge: Qualitative data for instruction can be hard to find, CAQDAS data formats are proprietary as are all leading tools. In this “train the trainer” workshop, we show how instructors can leverage the recent opening of qualitative research infrastructure and build an effective training around open qualitative data (as can be found in the Qualitative Data Repository), open standards (here, the REFI-QDA standard for the exchange of qualitative data projects), and open tools (the open source QualCoder software).

The workshop follows the structure of an “Introduction to Computer Assisted Qualitative Data Analysis” workshop, but focuses on the technical backgrounds and pedagogical issues that instructors face when teaching such a course. We begin with an introduction to the Qualitative Data Repository, the REFI-QDA format, and the QualCoder software. We discuss identifying qualitative data for teaching a workshop as well as common challenges in setting up and working with the QualCoder tool. We then jointly engage in an abbreviated version of a set of qualitative data coding exercises that highlights good practices in teaching the coding of qualitative data. The workshop concludes with some considerations for sharing coded qualitative data.

The target audience for the workshop are principally data professionals with some familiarity with qualitative research who are considering providing support for qualitative researchers. That said, the workshop does not have any pre-requirements and is also open to adventurous beginners interested in learning about qualitative data analysis and its developing open infrastructure.

**Workshop Learning Objectives:**

After participating in this workshop participants
- will be able to find and identify open qualitative data for teaching
- will be able to use, and if they have prior experience, teach with the FLOSS QualCoder software
- will know how to identify and address common challenges in teaching introductory qualitative data analysis classes using open tools and data.

**Workshop Prerequisites:**

None, though some familiarity with qualitative data analysis will benefit participants.

**Workshop Setting:**

Classroom - participants bring laptops

**Workshop Computer Requirements:**

The workshop relies exclusively on open tools and softwares that participants can install on their own laptops for free. As installation on Mac can be challenging, we will provide support for that prior to the workshop.

### 3: AI-enabled data practices for metadata discovery and access: Best practices for developing training data

Continued investment into new and existing data collection infrastructures (such as surveys and smart data), highlights the growing need for creation of efficient, robust and scalable data resources which help researchers find and access data. Recent advances in artificial intelligence (AI) methods to facilitate automatic analysis of large text collections provides a unique opportunity at the intersection of computational techniques and research methodologies for the development of data resources that are able to meet the current and future needs of the research community.

With the widening application of AI and machine learning (ML) pipelines for processing large text corpora, this workshop focuses on a fundamental prerequisite before setting up any pipeline for downstream tasks: the Dataset. It is a common perception that ML models are data hungry and require a vast amount of data to enhance model performance. While understandable, this perception can sometimes overshadow the importance of data quality. In collaboration with CLOSER, this workshop will cover a typical “packaging” of data to train and evaluate models. The workshop will explore various aspects that contribute towards good practice for creating quality training datasets, including exploratory data analysis, selection of evaluation metrics, model selection and model evaluation.

Conventionally, models are evaluated quantitatively, as represented by the appropriate metrics, and qualitatively. While it might be tedious to qualitatively analyse all the samples, random sampling could be problematic. In the section covering model evaluation, workshop participants will be introduced to the problem of data biases and gaps. By bridging technological approaches with social science research needs, this workshop offers an exploration of data transformation techniques that enhance research reproducibility and computational analysis capabilities.

**Workshop Learning Objectives:**

- Gain knowledge of best practice in creating good training data for machine learning models.
- Identify and explain relevant evaluation metrics.
- Analyse instances of bias in training data.

**Workshop Length:**

Half Day 3-Hour Session

**Workshop Setting:**

Classroom - participants bring laptops

### 4: Learn how to create synthetic data

The workshop will discuss the use of synthetic data for disclosure control. It will include discussions of what and how synthetic data can be used. Different types of synthetic data will be discussed. A practical session on how to create low-fidelity synthetic data, using the R package synthpop, will be included in the workshop and participants will be guided as to how they might proceed to create synthetic data with greater utility. The workshop will also include discussions of how to assess synthetic data for utility and disclosure control.

We will also review examples of cases where synthetic data has been released to the public.

**Workshop Learning Objectives:**

- To understand the ways that synthetic data can be used to widen access to sensitive data
- To learn how to use the R package synthpop to create low fidelity synthetic data

**Workshop Prerequisites:**

- A laptop with R installed
- Familiarity with the R programming language

**Workshop Setting:**

Classroom - participants bring laptops

**Workshop Computer Requirements:**

Participants can download the R package and all software and data required.

## Afternoon Workshops

### 5: Embedding sensitive data management best practice in institutional workflows

This half-day workshop will provide an overview of approaches to management of sensitive research data based on the collective experience of GW4 (https://gw4.ac.uk/) research data management support staff at the University of Bath, University of Bristol, Cardiff University, and University of Exeter, and how these approaches may be adapted for different institutional contexts. This will include design of consent forms, data management plans, and data sharing strategies, and how to embed this within the context of individual institutional workflows and policy frameworks.

The workshop is structured as a train-the-trainer package, developed from material previously delivered as part of the UKRN Train-the-Trainer programme (https://www.ukrn.org/training/data-sharing-from-planning-to-publishing-8-may/). It will include both subject matter content and workshop/training delivery design.

In the first part of the workshop we will deliver current GW4 best practice on design of consent forms to enable data sharing, data management planning for studies involving sensitive data, and strategies to enable sharing of sensitive data.

Following this, there will be discussion of pedagogic approaches to delivering training on these topics. The last part of the workshop will be devoted to developing content based on the topics covered in the earlier parts – there will be space for attendees to develop their own training materials tailored to their own institutional contexts. Attendees will be able to design and test content blocks and receive feedback from peers and workshop organisers.

**Workshop Learning Objectives:**

Objectives
- Understand the approaches for sensitive data management used at all four GW4 institutions
- Explore how decisions in the research design and consenting documents affect data sharing at the project’s completion
- Develop their own guidance and training materials, suited to their specific institutional context and infrastructure
- Share challenges and generate solutions for sensitive data management with other sector experts

**Workshop Prerequisites:**

This workshop is aimed at repository managers, data librarians, research governance/research support staff, and academics working with sensitive data. It may also be relevant for funding bodies that develop their own training. Familiarity with the general principles of research data management is required, but experience of sensitive data management is not.

**Workshop Setting:**

Classroom - discussion style/movable furniture

**Workshop Computer Requirements:**

Projector or display compatible with Windows 11 laptops and HDMI connectors.

### 6: The Art of Transcription: Using open-source tools to optimize transcription processes

One of the most significant challenges when working with qualitative data is the substantial amount of time and resources required to prepare text for analysis and sharing. A key step in this preparation is the process of converting audio files into text, or transcription. This task involves multiple elements, such as speaker segmentation, tagging, editing, anonymization, and notation. Each of these steps is essential for producing accurate, high-quality transcripts, but can also create substantial barriers to the efficient curation of qualitative data. In short, the effort involved in completing a full and accurate transcription can often slow down the overall workflow and limit the accessibility of data.

However, despite these challenges, complete transcriptions are essential for ensuring data privacy, enabling secure data sharing, and maximizing the reuse and value of qualitative collections. When done well, transcription makes it possible to share sensitive research data in a way that respects confidentiality and privacy while making the data more accessible to other researchers. As such, transcription is just as vital for the curation and long-term management of qualitative data as it is for research analysis itself.

This workshop is designed for individuals working with both audio and text data who are seeking solutions to streamline their transcription workflows. It will focus on exploring how open-source tools can ease the burden of transcription, notation, summarization, and anonymization. Participants will learn how to build a semi-automated curation pipeline that efficiently converts audio files into shareable, anonymized transcripts. They will also have the opportunity to discuss how these tools can be integrated into existing research workflows, helping to improve the efficiency and quality of transcriptions. By leveraging these powerful open-source tools, researchers can optimize their transcription process, reduce the workload involved, and critically evaluate the choices they make when handling qualitative data.

**Workshop Learning Objectives:**

By the end of this course, participants will be able to:

1. Understand and explain the differences in transcription approaches;
2. Identify key features of a transcript;
and
3. Effectively use online tools and software to produce a high-quality, accurate, qualitative transcript.

**Workshop Prerequisites:**

None

**Workshop Setting:**

Classroom - participants bring laptops

**Workshop Computer Requirements:**

Whisper, FAMTAFOS (or Textwash), and web browser (with Wifi connection)

### 7: Fundamentals of MAST & IDEAL Metadata

Fundamentals of MAST & IDEAL Metadata is a short-course in the MAST/IDEAL Methodology, a pragmatic framework for building understanding and organisational support for data management through good processes rather than specific tools or standards.

The MAST/IDEAL Methodology is a standards-agnostic and tools-independent approach that supports existing frameworks such as the Data Management Body of Knowledge and DDI Data Lifecycle, which talk about what data practitioners must do, by providing a step-by-step guide on how practitioners can develop skills and culture to perform these tasks.

Learning objectives for the course include:

- Understanding the principles of the MAST (Metadata-Analysis-Standards-Teamwork) Manifesto
- Understand the stages in the IDEAL (Inventory-Document-Endorse-Audit-Leadership) Framework
- How to communicate and promote the importance of metadata for a range of audiences - from researcher and data staff to the administrative and executive level
- How to identify skills for specialist teams to develop a culture of metadata peer review to support modern data governance
- How to communicate steps and focus effort in data documentation and governance to maximise success in data initiatives
- How to identify appropriate metadata standards and software for successful data projects

This course will be delivered in a practioner-led group session, including:

- Peer discussions on case studies highlighting existing challenges in change management and education in data governance
- Examination of sample templates for documenting data and how to adopt these for existing projects
- Peer discussions on the use of MAST & IDEAL for the selection of data standards and software

As an emerging framework, following the course participants will be invited to provide feedback on the methods and frameworks presented to gather data on ongoing research into the framework. Participants will also be invited to complete an online quiz to receive a digital micro-certification to promote their own experience and knowledge.

**Workshop Learning Objectives:**

- How to communicate and promote the importance of metadata for a range of audiences - from researcher and data staff to the administrative and executive level
- How to identify skills for specialist teams to develop a culture of metadata peer review to support modern data governance
- How to communicate steps and focus effort in data documentation and governance to maximise success in data initiatives
- How to identify appropriate metadata standards and software for successful data projects

**Workshop Setting:**

Classroom - discussion style/movable furniture

### 8: An Introduction to Census Data with R Using the "tidycensus" Package

Data Librarians and social science information professionals must often use Census data, whether it is for their own projects, or in order to assist researchers or students with their empirical projects. The process of extracting, querying, analyzing, and visualizing these data can be complex, however; as a result, academic libraries often subscribe to commercial software products that make this process more straightforward. However, these solutions are expensive, and may not be interoperable with the tools and platforms that researchers typically use to conduct empirical research.

In recent years, robust open-source software packages to work with Census data (using the Census Bureau's API) have been developed; these packages provide efficient, cost-effective, and user-friendly pathways to exploring Census data. This workshop provides a hands-on introduction to one such package, namely, the "tidycensus" package, which allows users to interact with the Census API using the R programming language. Participants will learn how to identify, locate, and extract data from the Census (and associated datasets) using "tidycensus", and subsequently analyze these data using tools from the R "tidyverse." 

Completing the workshop will empower participants to use "tidycensus" in their own research, as well as in their consultations with students and researchers who must work with Census data in their projects. Please note that while our focus will be on the United States Census, tools to extract and work with Census and demographic data from other countries using the R programming language will also be introduced.

**Workshop Learning Objectives:**

Participants will learn how to:

1) Query and extract Census American Community Survey datasets to their local R environment using the "tidycensus" package
2) Wrangle and process Census datasets to prepare them for analysis
3) Create basic maps and visualizations using the census data they extract

**Workshop Prerequisites:**

Participants should have some prior exposure to the R programming language and the "tidyverse", though the workshop will not presuppose extensive prior experience. The completion of a "Carpentries" or Carpentries-style introductory workshop on the R programming language would be sufficient preparation to participate in this workshop.

**Workshop Computer Requirements:**

Participants should attend with their personal laptops, with R and R Studio installed. Before the workshop, they must also apply for a Census API Key (https://api.census.gov/data/key_signup.html); typically, an API key will be issued immediately upon applying. An API key will be needed to participate in the workshop.



<!--
## Workshop registration

You may now register for IASSIST & CARTO 2024 Workshops. Workshops will be held on Tuesday May 28 at Dalhousie University. More information about room locations will be provided in advance. 

It is only possible to attend one workshop in the morning and another in the afternoon. Workshop registration is open to all, you do not have to register for the full conference to register for a workshop. 

<br />
<a class="btn btn-template-main" href="https://iassistdata.info/event-5601452">Register for IASSIST & CARTO Workshop <span class="fas fa-external-link-alt"></span></a>
<br /><br />

**Please note** that due to a system limitation, you can only register for 1 workshop at a time. If you wish to register for two workshops (both morning and afternoon), complete the workshop registration form twice. We apologize for any inconvenience.

### Information for participants

Once you have registered for a workshop, please see this [information document <span class="fas fa-external-link-alt"></span>](https://tinyurl.com/iassist24workshops) with information regarding what you should bring with you to the workshop. Instructors may also contact participants prior to the workshop with more details. 

## Workshops available

### Morning Workshops (9:00-12:00)

#### Introduction to Data Analysis with Python
##### Tim Dennis, Cody Hennesy

This Python workshop uses hands-on coding to introduce programming for library and information workers with little or no previous programming experience. The lesson utilizes open datasets to model reproducible workflows for data analysis, with a focus on helping learners apply and work with fundamental Python concepts such as data types, functions, loops, and libraries. The open-source Library Carpentry Python lesson that we’ll use to teach this workshop  is currently undergoing a major redesign, and will use the JupyterLab environment along with Pandas dataframes to explore and generate descriptive statistics from a quantitative dataset of library usage data. The workshop provides a basic introduction for those working with metadata, citations, and quantitative data, and serves as a great first step for folks hoping to continue to build skills to access, clean, analyze, and visualize data with Python.

#### Coding Qualitative Data: The Methods (using your Mind) and the Mechanics (using Taguette)
##### Mandy Swygart-Hobaugh

This hands-on workshop introduces participants to both the methodological knowledge and the mechanical skills to collaboratively code qualitative data. The workshop will open with a "mini-methods" talk about the broader methodological and epistemological considerations of qualitative research, including discussion of qualitative research methods, qualitative data types, and analytical coding of qualitative data [approximately 75 minutes total of the 3-hour workshop time]. Then participants will apply the mini-methods knowledge by hands-on coding an interview transcript collaboratively using Taguette, a free and open-source qualitative research tool.

NOTE: This workshop is an adaptation of curriculum unit developed for a Fall 2023 Georgia State University (GSU) experiential learning lab course – "Tackling Food Insecurity: A Public Interest Data Literacy (PIDLit) Learning Lab" – taught by members of the GSU Library’s Research Data Services (RDS) Department. Methods examples and the coding exercise data relate to individuals and communities experiencing food insecurity.

#### Let's create data schemas!
##### Michelle Edwards

A key component to any research project is data and making that data FAIR-compliant is our ultimate goal.  However, many researchers today are creating minimal documentation of their data, maybe just enough to pass the requirements of the data management plan (DMP), and maybe the requirements of a data repository.  In many situations, researchers may be relying on data repositories or archives to create the necessary documentation to make their data FAIR or are just not concerned about it.

At the University of Guelph, we are encouraging and enabling our researchers to create data schemas at the start of their project (before data collection).  The schemas are created using our Semantic Engine and the newly developed Overlays Capture Architecture.  Data schemas may be stored, shared, edited, and are all uniquely identified.  Imagine a large 5yr research project using the same data schemas and data entry forms across all their students and research associates?  The time that can be saved when pooling all the data for analysis is immense.  Imagine being able to confirm that the data you just discovered was indeed the one created by the Project Lead 10 years ago and not altered in any way?

This workshop will walk you through the development of the Semantic Engine tools and provide hands-on experience creating your own data schemas.  We will work with a sample dataset and demonstrate how we can build the layers of a schema, how they are uniquely identified, how you can share them, build upon an existing schema, and how we can create a data entry Excel file for the project.  We will close the workshop with a discussion about current and future development and possible uses within the broader research ecosystem - just think how AI could use these schemas?

#### Navigating the Shoals of Moral Hazards as a Data Ethics Privateer

**\*Not available\***

##### Cameron Tuai

Where the ocean of data meet the shores of analytics lay many a treacherous shoals of moral hazard.  Navigating these hazards requires both a sound ethical map and a grasp of the peculiarity of data-oriented moral dilemmas. Learn the ropes of basic western ethical philosophy, plot the characteristics of data-oriented moral hazards, and lay sail to ethical point unknown.  Register now and you'll cruise the seas for data gold, you'll fire no guns and shed no tears, register now and become a data ethic's privateer. (see Rogers, Barrett's Privateers)

Building on five years of teaching a three credit applied ethics and technology course, this hands on workshop will focus providing participants with the following skill sets:

1. Practical knowledge of the ethical philosophies of Kant; Utility Theory; and Social Contract Theory
2. Ability to identify and document the implications of technology based moral dilemmas in terms of being opaque, muddled, and existing within a policy vacuum
3. Application of ethical theory to analyze data-oriented moral dilemma
4. Limitations of standard ethical theory and an exploration of Ethics of Care as an alternative approach

As a practice oriented workshop, participants will apply technology based ethical theory to analyze a privacy based moral dilemma drawn from the 2023 SPARK analysis of the Web of Science suite of products.  This privacy-oriented case study will provide participants with both an opportunity to apply ethical theory and a specific framework for describing the hazards of privacy in terms of the Panopticon; moral capital; self-determination.

The goal of the workshop is to enlist a crew of data privateers to cruise the seas of moral hazard and bring back the riches of data that rightfully belong to the common good.


#### Fundamentals of MAST & IDEAL Metadata
##### Sam Spencer

SPONSORED BY ARISTOTLE. Fundamentals of MAST & IDEAL Metadata is a short-course in the MAST/IDEAL Methodology, a pragmatic framework for building understanding and organisational support for data management through good processes rather than specific tools or standards.

The MAST/IDEAL Methodology is a standards-agnostic and tools-independent approach that supports existing frameworks such as the Data Management Body of Knowledge and DDI Data Lifecycle, which talk about what data practitioners must do, by providing a step-by-step guide on how practitioners can develop skills and culture to perform these tasks.

Learning objectives for the course include:

- Understanding the principles of the MAST (Metadata-Analysis-Standards-Teamwork) Manifesto 
- Understand the stages in the IDEAL (Inventory-Document-Endorse-Audit-Leadership) Framework
- How to communicate and promote the importance of metadata for a range of audiences - from researcher and data staff to the administrative and executive level
- How to identify skills for specialist teams to develop a culture of metadata peer review to support modern data governance
- How to communicate steps and focus effort in data documentation and governance to maximise success in data initiatives
- How to identify appropriate metadata standards and software for successful data projects

This course will be delivered in a practioner-led group session, including:

- Peer discussions on case studies highlighting existing challenges in change management and education in data governance
- Examination of sample templates for documenting data and how to adopt these for existing projects
- Peer discussions on the use of MAST & IDEAL for the selection of data standards and software

As an emerging framework, following the course participants will be invited to provide feedback on the methods and frameworks presented to gather data on ongoing research into the framework. Participants will also be invited to complete an online quiz to receive a digital micro-certification to promote their own experience and knowledge.

### Afternoon Workshops (13:00-16:00)

#### Leveraging ChatGPT for Research: Effective Data Gathering Techniques

**\*workshop cancelled\***


##### Patience Chewachong Akih

The purpose of the workshop is to give a comprehensive understanding of the data gathering techniques used in research. The session will address ethical considerations and discuss how to get the best results from data gathering. The potential for aiding research is shown in the introduction to the workshop. Practical insights into its real-world applications are provided by the diverse strategies and methodologies it explores. The importance of ethical considerations is emphasized in the workshop. Discussion on ethical implications and best practices will be important. Participants will learn how to gather data through hands-on exercises and simulations. The workshop will focus on data quality and reliability, offering insights to enhance accuracy and relevance, and fostering a collaborative learning environment. Resources, toolkits, and references will be at the end of the workshop. The goal is to equip participants with comprehensive knowledge and practical skills that will allow them to effectively use ChatGPT for data gathering in research contexts.


#### Understanding Data Anonymization
##### Kristi Thompson

Data curators should have a basic understanding of data anonymization so they can support safe sharing of sensitive data and avoid sharing data that accidentally violates confidentiality. This workshop will consist of a lecture followed by a session using R. The first half will cover the mathematical and theoretical underpinnings of guaranteed data anonymization. Topics covered include an overview of identifiers and quasi-identifiers, an introduction to k-anonymity, a look at some cases where k-anonymity breaks down, and a discussion of various enhancements of k-anonymity.

The second half will walk participants through some steps to assess the disclosure risk of a dataset and anonymize it using R and the R package SDCMicro.

Much of the academic material looking at data anonymization is quite abstract and aimed at computer scientists, while material aimed at data curators does not always consider recent developments. This session is intended to help bridge the gap.

#### Open Data and QGIS for Strengthening Data and Geographic Competencies and Humanitarian Interventions
##### Ann James

Key concepts and technical skills in data and geographic literacies play a vital role in the effective communication of research and analysis of humanitarian importance conducted by individuals employed in academia, nonprofits, and government teams in North America and elsewhere. Such literacies include an individual’s ability to select, clean, analyze, visualize, critique, and interpret relevant open geospatial datasets. It also includes an ability to understand and critically apply fundamental geographic principles which underlie geospatial technologies. 

This workshop engages open data and open source GIS software to provide participants an introduction to key concepts and technical skills in data and geographic literacies. During the workshop, participants will become familiar with key geographic concepts, especially the map, geographic place, projections, and coordinate reference systems. They will gain hands-on experience accessing, visualizing, and interpreting a geospatial dataset representing sensitive subjects of relevance to public health within the US-Mexico international border region. Through their participation, attendees’ may also enhance their awareness and knowledge of open educational resources, international migration as a public health issue, and opportunities for humanitarian interventions in borderlands, such as improvements in an ability to access information, clean water, food, and shelter.

#### Data Curation Fundamentals
##### Mikala Narlock, Sophia Lafferty-Hess, Erin Clary, Amber Leahey, Meghan Goodchild, Tamanna Moharana, Robyn Stobbs 

Data curation is a key component of the data sharing and publication process, during which data professionals review a dataset, code, and related outputs to ensure that data are findable, accessible, interoperable, and reusable (FAIR) and incorporates ethical curation considerations. Data curation enables data discovery and access, maintains data quality and authenticity, adds value, and provides for re-use over time through activities including open and controlled access, archiving, metadata creation, digital preservation, and file format transformation. There are many additional activities encompassed by the term data curation – which can be daunting for a novice to understand and apply in a meaningful way.

The IASSIST & CARTO 2024 proposed Data Curation Network (DCN) and Digital Research Alliance of Canada co-hosted Data Curation Fundamentals training workshop will provide attendees with a framework for getting started with data curation, including hands-on practical curation training using various data formats. Using the DCN CURATE(D) workflow (z.umn.edu/curate) and the Canadian bilingual (English and French) CURATION framework, attendees will learn practical curation techniques that can be applied across research disciplines. Provided by members of the DCN Education Committee, and the Alliance’s Curation Expert Group (CEG) and invited speakers, this workshop will leverage both active learning opportunities, using example datasets, as well as discussions in an inclusive peer-to-peer learning environment. This established curriculum has been used for both in-person and virtual learning opportunities, with overwhelming success. This workshop has been taught in the United States, adapted and extended for use in Canada, and we are eager to bring our curriculum to the international community.

#### Research Reproducibility and Data Management with R
##### Briana Wham

In the rapidly evolving research landscape, characterized by the increasing prevalence of open science practices, data intensive methodologies, and computational approaches, ensuring the reproducibility and transparency of research outcomes is vital. The aim of this hands-on workshop is to empower participants with the knowledge and practical skills required to implement data management practices and cultivate reproducible research practices within the RStudio environment. Leveraging the capabilities of R and RStudio, the workshop will delve into various aspects of research reproducibility and data management, emphasizing literate programming, project workflows, and an array of R packages to support data management and reproducibility efforts. Participants can expect to gain an increased understanding of how R and RStudio can serve as instrumental tools in fostering transparent, reproducible, and reusable research.
-->