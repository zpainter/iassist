+++
year = "2025"
draft = "false"
title = "IASSIST 2025 - Workshops"
location = "Bristol, UK"
banner = "/img/conferences/annual/bristol2025-socialmedia.jpg"
type = "conference-2025"
+++

We have a fantastic programme of events scheduled for this year’s IASSIST soon to be announced.

<!--
## Workshop registration

You may now register for IASSIST & CARTO 2024 Workshops. Workshops will be held on Tuesday May 28 at Dalhousie University. More information about room locations will be provided in advance. 

It is only possible to attend one workshop in the morning and another in the afternoon. Workshop registration is open to all, you do not have to register for the full conference to register for a workshop. 

<br />
<a class="btn btn-template-main" href="https://iassistdata.info/event-5601452">Register for IASSIST & CARTO Workshop <span class="fas fa-external-link-alt"></span></a>
<br /><br />

**Please note** that due to a system limitation, you can only register for 1 workshop at a time. If you wish to register for two workshops (both morning and afternoon), complete the workshop registration form twice. We apologize for any inconvenience.

### Information for participants

Once you have registered for a workshop, please see this [information document <span class="fas fa-external-link-alt"></span>](https://tinyurl.com/iassist24workshops) with information regarding what you should bring with you to the workshop. Instructors may also contact participants prior to the workshop with more details. 

## Workshops available

### Morning Workshops (9:00-12:00)

#### Introduction to Data Analysis with Python
##### Tim Dennis, Cody Hennesy

This Python workshop uses hands-on coding to introduce programming for library and information workers with little or no previous programming experience. The lesson utilizes open datasets to model reproducible workflows for data analysis, with a focus on helping learners apply and work with fundamental Python concepts such as data types, functions, loops, and libraries. The open-source Library Carpentry Python lesson that we’ll use to teach this workshop  is currently undergoing a major redesign, and will use the JupyterLab environment along with Pandas dataframes to explore and generate descriptive statistics from a quantitative dataset of library usage data. The workshop provides a basic introduction for those working with metadata, citations, and quantitative data, and serves as a great first step for folks hoping to continue to build skills to access, clean, analyze, and visualize data with Python.

#### Coding Qualitative Data: The Methods (using your Mind) and the Mechanics (using Taguette)
##### Mandy Swygart-Hobaugh

This hands-on workshop introduces participants to both the methodological knowledge and the mechanical skills to collaboratively code qualitative data. The workshop will open with a "mini-methods" talk about the broader methodological and epistemological considerations of qualitative research, including discussion of qualitative research methods, qualitative data types, and analytical coding of qualitative data [approximately 75 minutes total of the 3-hour workshop time]. Then participants will apply the mini-methods knowledge by hands-on coding an interview transcript collaboratively using Taguette, a free and open-source qualitative research tool.

NOTE: This workshop is an adaptation of curriculum unit developed for a Fall 2023 Georgia State University (GSU) experiential learning lab course – "Tackling Food Insecurity: A Public Interest Data Literacy (PIDLit) Learning Lab" – taught by members of the GSU Library’s Research Data Services (RDS) Department. Methods examples and the coding exercise data relate to individuals and communities experiencing food insecurity.

#### Let's create data schemas!
##### Michelle Edwards

A key component to any research project is data and making that data FAIR-compliant is our ultimate goal.  However, many researchers today are creating minimal documentation of their data, maybe just enough to pass the requirements of the data management plan (DMP), and maybe the requirements of a data repository.  In many situations, researchers may be relying on data repositories or archives to create the necessary documentation to make their data FAIR or are just not concerned about it.

At the University of Guelph, we are encouraging and enabling our researchers to create data schemas at the start of their project (before data collection).  The schemas are created using our Semantic Engine and the newly developed Overlays Capture Architecture.  Data schemas may be stored, shared, edited, and are all uniquely identified.  Imagine a large 5yr research project using the same data schemas and data entry forms across all their students and research associates?  The time that can be saved when pooling all the data for analysis is immense.  Imagine being able to confirm that the data you just discovered was indeed the one created by the Project Lead 10 years ago and not altered in any way?

This workshop will walk you through the development of the Semantic Engine tools and provide hands-on experience creating your own data schemas.  We will work with a sample dataset and demonstrate how we can build the layers of a schema, how they are uniquely identified, how you can share them, build upon an existing schema, and how we can create a data entry Excel file for the project.  We will close the workshop with a discussion about current and future development and possible uses within the broader research ecosystem - just think how AI could use these schemas?

#### Navigating the Shoals of Moral Hazards as a Data Ethics Privateer

**\*Not available\***

##### Cameron Tuai

Where the ocean of data meet the shores of analytics lay many a treacherous shoals of moral hazard.  Navigating these hazards requires both a sound ethical map and a grasp of the peculiarity of data-oriented moral dilemmas. Learn the ropes of basic western ethical philosophy, plot the characteristics of data-oriented moral hazards, and lay sail to ethical point unknown.  Register now and you'll cruise the seas for data gold, you'll fire no guns and shed no tears, register now and become a data ethic's privateer. (see Rogers, Barrett's Privateers)

Building on five years of teaching a three credit applied ethics and technology course, this hands on workshop will focus providing participants with the following skill sets:

1. Practical knowledge of the ethical philosophies of Kant; Utility Theory; and Social Contract Theory
2. Ability to identify and document the implications of technology based moral dilemmas in terms of being opaque, muddled, and existing within a policy vacuum
3. Application of ethical theory to analyze data-oriented moral dilemma
4. Limitations of standard ethical theory and an exploration of Ethics of Care as an alternative approach

As a practice oriented workshop, participants will apply technology based ethical theory to analyze a privacy based moral dilemma drawn from the 2023 SPARK analysis of the Web of Science suite of products.  This privacy-oriented case study will provide participants with both an opportunity to apply ethical theory and a specific framework for describing the hazards of privacy in terms of the Panopticon; moral capital; self-determination.

The goal of the workshop is to enlist a crew of data privateers to cruise the seas of moral hazard and bring back the riches of data that rightfully belong to the common good.


#### Fundamentals of MAST & IDEAL Metadata
##### Sam Spencer

SPONSORED BY ARISTOTLE. Fundamentals of MAST & IDEAL Metadata is a short-course in the MAST/IDEAL Methodology, a pragmatic framework for building understanding and organisational support for data management through good processes rather than specific tools or standards.

The MAST/IDEAL Methodology is a standards-agnostic and tools-independent approach that supports existing frameworks such as the Data Management Body of Knowledge and DDI Data Lifecycle, which talk about what data practitioners must do, by providing a step-by-step guide on how practitioners can develop skills and culture to perform these tasks.

Learning objectives for the course include:

- Understanding the principles of the MAST (Metadata-Analysis-Standards-Teamwork) Manifesto 
- Understand the stages in the IDEAL (Inventory-Document-Endorse-Audit-Leadership) Framework
- How to communicate and promote the importance of metadata for a range of audiences - from researcher and data staff to the administrative and executive level
- How to identify skills for specialist teams to develop a culture of metadata peer review to support modern data governance
- How to communicate steps and focus effort in data documentation and governance to maximise success in data initiatives
- How to identify appropriate metadata standards and software for successful data projects

This course will be delivered in a practioner-led group session, including:

- Peer discussions on case studies highlighting existing challenges in change management and education in data governance
- Examination of sample templates for documenting data and how to adopt these for existing projects
- Peer discussions on the use of MAST & IDEAL for the selection of data standards and software

As an emerging framework, following the course participants will be invited to provide feedback on the methods and frameworks presented to gather data on ongoing research into the framework. Participants will also be invited to complete an online quiz to receive a digital micro-certification to promote their own experience and knowledge.

### Afternoon Workshops (13:00-16:00)

#### Leveraging ChatGPT for Research: Effective Data Gathering Techniques

**\*workshop cancelled\***


##### Patience Chewachong Akih

The purpose of the workshop is to give a comprehensive understanding of the data gathering techniques used in research. The session will address ethical considerations and discuss how to get the best results from data gathering. The potential for aiding research is shown in the introduction to the workshop. Practical insights into its real-world applications are provided by the diverse strategies and methodologies it explores. The importance of ethical considerations is emphasized in the workshop. Discussion on ethical implications and best practices will be important. Participants will learn how to gather data through hands-on exercises and simulations. The workshop will focus on data quality and reliability, offering insights to enhance accuracy and relevance, and fostering a collaborative learning environment. Resources, toolkits, and references will be at the end of the workshop. The goal is to equip participants with comprehensive knowledge and practical skills that will allow them to effectively use ChatGPT for data gathering in research contexts.


#### Understanding Data Anonymization
##### Kristi Thompson

Data curators should have a basic understanding of data anonymization so they can support safe sharing of sensitive data and avoid sharing data that accidentally violates confidentiality. This workshop will consist of a lecture followed by a session using R. The first half will cover the mathematical and theoretical underpinnings of guaranteed data anonymization. Topics covered include an overview of identifiers and quasi-identifiers, an introduction to k-anonymity, a look at some cases where k-anonymity breaks down, and a discussion of various enhancements of k-anonymity.

The second half will walk participants through some steps to assess the disclosure risk of a dataset and anonymize it using R and the R package SDCMicro.

Much of the academic material looking at data anonymization is quite abstract and aimed at computer scientists, while material aimed at data curators does not always consider recent developments. This session is intended to help bridge the gap.

#### Open Data and QGIS for Strengthening Data and Geographic Competencies and Humanitarian Interventions
##### Ann James

Key concepts and technical skills in data and geographic literacies play a vital role in the effective communication of research and analysis of humanitarian importance conducted by individuals employed in academia, nonprofits, and government teams in North America and elsewhere. Such literacies include an individual’s ability to select, clean, analyze, visualize, critique, and interpret relevant open geospatial datasets. It also includes an ability to understand and critically apply fundamental geographic principles which underlie geospatial technologies. 

This workshop engages open data and open source GIS software to provide participants an introduction to key concepts and technical skills in data and geographic literacies. During the workshop, participants will become familiar with key geographic concepts, especially the map, geographic place, projections, and coordinate reference systems. They will gain hands-on experience accessing, visualizing, and interpreting a geospatial dataset representing sensitive subjects of relevance to public health within the US-Mexico international border region. Through their participation, attendees’ may also enhance their awareness and knowledge of open educational resources, international migration as a public health issue, and opportunities for humanitarian interventions in borderlands, such as improvements in an ability to access information, clean water, food, and shelter.

#### Data Curation Fundamentals
##### Mikala Narlock, Sophia Lafferty-Hess, Erin Clary, Amber Leahey, Meghan Goodchild, Tamanna Moharana, Robyn Stobbs 

Data curation is a key component of the data sharing and publication process, during which data professionals review a dataset, code, and related outputs to ensure that data are findable, accessible, interoperable, and reusable (FAIR) and incorporates ethical curation considerations. Data curation enables data discovery and access, maintains data quality and authenticity, adds value, and provides for re-use over time through activities including open and controlled access, archiving, metadata creation, digital preservation, and file format transformation. There are many additional activities encompassed by the term data curation – which can be daunting for a novice to understand and apply in a meaningful way.

The IASSIST & CARTO 2024 proposed Data Curation Network (DCN) and Digital Research Alliance of Canada co-hosted Data Curation Fundamentals training workshop will provide attendees with a framework for getting started with data curation, including hands-on practical curation training using various data formats. Using the DCN CURATE(D) workflow (z.umn.edu/curate) and the Canadian bilingual (English and French) CURATION framework, attendees will learn practical curation techniques that can be applied across research disciplines. Provided by members of the DCN Education Committee, and the Alliance’s Curation Expert Group (CEG) and invited speakers, this workshop will leverage both active learning opportunities, using example datasets, as well as discussions in an inclusive peer-to-peer learning environment. This established curriculum has been used for both in-person and virtual learning opportunities, with overwhelming success. This workshop has been taught in the United States, adapted and extended for use in Canada, and we are eager to bring our curriculum to the international community.

#### Research Reproducibility and Data Management with R
##### Briana Wham

In the rapidly evolving research landscape, characterized by the increasing prevalence of open science practices, data intensive methodologies, and computational approaches, ensuring the reproducibility and transparency of research outcomes is vital. The aim of this hands-on workshop is to empower participants with the knowledge and practical skills required to implement data management practices and cultivate reproducible research practices within the RStudio environment. Leveraging the capabilities of R and RStudio, the workshop will delve into various aspects of research reproducibility and data management, emphasizing literate programming, project workflows, and an array of R packages to support data management and reproducibility efforts. Participants can expect to gain an increased understanding of how R and RStudio can serve as instrumental tools in fostering transparent, reproducible, and reusable research.
-->